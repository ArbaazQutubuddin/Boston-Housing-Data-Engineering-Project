import os
from pathlib import Path
from datetime import datetime
from airflow import DAG
from airflow.models.baseoperator import chain
from airflow.operators.dummy_operator import DummyOperator
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryCreateEmptyDatasetOperator,
    BigQueryDeleteDatasetOperator,
    BigQueryCreateEmptyTableOperator,
)
from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator

base_path = Path(__file__).parents[1]
data_file = os.path.join(base_path, "data", "boston_housing_dataset.csv")
ge_root_dir = os.path.join(base_path, "config", "ge")

# In a production DAG, the global variables below should be stored as Airflow
# or Environment variables.
PROJECT_ID = os.environ.get("GCP_PROJECT_ID")
gcp_bucket = os.environ.get("GCP_GCS_BUCKET")

bq_dataset = "arbaazdeproject"
bq_table = "arbaazdeproject"

gcp_data_dest = "data/boston_housing_dataset.csv"

with DAG(
    "great_expectations.bigquery",
    description="Example DAG showcasing loading and data quality checking with BigQuery and Great Expectations.",
    doc_md=__doc__,
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    catchup=False,
) as dag:

    """
    #### BigQuery dataset creation
    Create the dataset to store the sample data tables.
    """
    create_dataset = BigQueryCreateEmptyDatasetOperator(
        task_id="create_dataset", dataset_id=bq_dataset
    )

    """
    #### Upload Boston housing data to GCS
    Upload the test data to GCS so it can be transferred to BigQuery.
    """
    upload_boston_data = LocalFilesystemToGCSOperator(
        task_id="upload_boston_data",
        src=data_file,
        dst=gcp_data_dest,
        bucket=gcp_bucket,
    )

    """
    #### Create Temp Table for GE in BigQuery
    """
    create_temp_table = BigQueryCreateEmptyTableOperator(
        task_id="create_temp_table",
        dataset_id=bq_dataset,
        table_id=f"{bq_table}_temp",
        schema_fields=[
            {"name": "CRIM", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "ZN", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "INDUS", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "CHAS", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "NOX", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "RM", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "AGE", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "DIS", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "RAD", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "TAX", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "PTRATIO", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "B", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "LSTAT", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "MEDV", "type": "FLOAT", "mode": "NULLABLE"},
        ],
    )

    """
    #### Transfer data from GCS to BigQuery
    Moves the data uploaded to GCS in the previous step to BigQuery, where
    Great Expectations can run a test suite against it.
    """
    transfer_boston_data = GCSToBigQueryOperator(
        task_id="boston_data_gcs_to_bigquery",
        bucket=gcp_bucket,
        source_objects=[gcp_data_dest],
        skip_leading_rows=1,
        destination_project_dataset_table="{}.{}".format(PROJECT_ID,bq_dataset, bq_table),
        schema_fields=[
            {"name": "CRIM", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "ZN", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "INDUS", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "CHAS", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "NOX", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "RM", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "AGE", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "DIS", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "RAD", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "TAX", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "PTRATIO", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "B", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "LSTAT", "type": "FLOAT", "mode": "NULLABLE"},
            {"name": "MEDV", "type": "FLOAT", "mode": "NULLABLE"},
        ],
        source_format="CSV",
        create_disposition="CREATE_IF_NEEDED",
        write_disposition="WRITE_TRUNCATE",
        allow_jagged_rows=True,
    )

    """
    #### Great Expectations suite
    Run the Great Expectations suite on the table.
    """
    ge_bigquery_validation_pass = GreatExpectationsOperator(
        task_id="ge_bigquery_validation_pass",
        data_context_root_dir=ge_root_dir,
        checkpoint_name='demo_taxi_pass_chk',
        return_json_dict=True
    )

    ge_bigquery_validation_fail = GreatExpectationsOperator(
        task_id="ge_bigquery_validation_fail",
        data_context_root_dir=ge_root_dir,
        checkpoint_name='demo_taxi_fail_chk',
        return_json_dict=True
    )

    """
    #### Delete test dataset and table
    Clean up the dataset and table created for the example.
    """
    '''delete_dataset = BigQueryDeleteDatasetOperator(
        task_id="delete_dataset",
        project_id=PROJECT_ID,
        dataset_id=bq_dataset,
        delete_contents=True,
        trigger_rule="all_done"
    )'''

    begin = DummyOperator(task_id="begin")
    end = DummyOperator(task_id="end", trigger_rule="all_done")

    chain(
        begin,
        create_dataset,
        create_temp_table,
        upload_boston_data,
        transfer_boston_data,
        [ge_bigquery_validation_pass, ge_bigquery_validation_fail],
        #delete_dataset,
        end,
    )
